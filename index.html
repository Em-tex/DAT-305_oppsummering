<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DAT305 - AI for Engineers</title>
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤–</text></svg>">
    
    <link rel="stylesheet" href="style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    
    <script>
    MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] }
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

    <nav class="navbar">
        <div class="brand"><i class="fas fa-robot"></i> DAT305 Prep</div>
        <ul class="nav-links">
            <li onclick="showSection('mod1', this)" class="active">Mod 1: Intro</li>
            <li onclick="showSection('mod2', this)">Mod 2: Math</li>
            <li onclick="showSection('mod3', this)">Mod 3: ML</li>
            <li onclick="showSection('mod4', this)">Mod 4: ANN</li>
            <li onclick="showSection('mod5', this)">Mod 5: Deep Learning</li>
            <li onclick="showSection('mod6', this)">Mod 6: NLP</li>
            <li onclick="showQuiz(this)" class="quiz-btn"><i class="fas fa-question-circle"></i> Exam Quiz</li>
        </ul>
    </nav>

    <main id="content-area">
        
        <section id="mod1" class="module active">
            <div class="header-banner">
                <h1>Module 1: Introduction to Machine Learning</h1>
                <p>Concepts, Types of Learning & Python Libraries</p>
            </div>

            <div class="slide">
                <h3>1.1 Fundamentals</h3>
                <p><strong>Machine Learning (ML)</strong> allows computers to learn from data and make decisions without explicit programming. Unlike traditional coding (Input + Rules = Output), ML uses (Input + Output = Rules).</p>
                
                <div class="grid-3">
                    <div class="card">
                        <h4><i class="fas fa-chalkboard-teacher"></i> Supervised Learning</h4>
                        <p>Learning from <span class="term" data-desc="Data containing both the input features (X) and the correct answer/label (Y).">Labeled Data</span>.</p>
                        <ul>
                            <li><strong>Regression:</strong> Predicting continuous values (e.g., Stock Price, Temperature).</li>
                            <li><strong>Classification:</strong> Predicting discrete categories (e.g., Spam/Ham, Cat/Dog).</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4><i class="fas fa-search"></i> Unsupervised Learning</h4>
                        <p>Learning from <strong>Unlabeled Data</strong> to find hidden structures.</p>
                        <ul>
                            <li><strong>Clustering:</strong> Grouping similar items (Customer Segmentation).</li>
                            <li><strong>Dimensionality Reduction:</strong> Simplifying complex data (PCA).</li>
                        </ul>
                    </div>
                    <div class="card">
                        <h4><i class="fas fa-gamepad"></i> Reinforcement Learning</h4>
                        <p>An agent interacts with an environment to maximize a <span class="term" data-desc="A feedback signal (score) used to reinforce good actions and punish bad ones.">Reward</span>.</p>
                        <p><em>Examples:</em> Robot navigation, AlphaGo, Self-driving cars.</p>
                    </div>
                </div>
            </div>

            <div class="slide">
                <h3>1.2 Python Libraries for ML</h3>
                <div class="tech-stack-visual">
                    <div class="tech-item" style="border-color: #f1c40f;">
                        <strong>NumPy</strong><br>Numerical Math, N-dim Arrays.
                    </div>
                    <div class="tech-item" style="border-color: #e67e22;">
                        <strong>Pandas</strong><br>DataFrames, Analysis, Time-series.
                    </div>
                    <div class="tech-item" style="border-color: #3498db;">
                        <strong>SciPy</strong><br>Scientific Computing, Optimization.
                    </div>
                    <div class="tech-item" style="border-color: #9b59b6;">
                        <strong>Scikit-Learn</strong><br>Algorithms: Regression, SVM, PCA, K-Means.
                    </div>
                </div>
                <p><strong>Workflow:</strong> Data Load (Pandas) &rarr; Preprocessing (NumPy) &rarr; Modeling (Scikit-Learn) &rarr; Evaluation.</p>
            </div>
        </section>

        <section id="mod2" class="module">
            <div class="header-banner">
                <h1>Module 2: Mathematics for AI</h1>
                <p>Linear Algebra, Data Representation & SVD/PCA</p>
            </div>
            
            <div class="slide">
                <h3>2.1 Linear Algebra Foundations</h3>
                <div class="grid-2">
                    <div>
                        <p><strong>Vector:</strong> A 1D array of numbers. Represents a single data point's features (e.g., [Height, Weight, Age]).</p>
                        <p><strong>Matrix:</strong> A 2D table of numbers. Rows = Samples, Columns = Features.</p>
                        <p><strong>Square Matrix:</strong> $N \times N$ dimensions (Same rows and cols).</p>
                        <div class="formula">
                            $$ \text{Dot Product: } \mathbf{a} \cdot \mathbf{b} = \sum a_i b_i $$
                        </div>
                        <p>If $\mathbf{a} \cdot \mathbf{b} = 0$, the vectors are <span class="term" data-desc="Perpendicular to each other. In statistics, this implies they are uncorrelated.">Orthogonal</span>.</p>
                    </div>
                    <div class="matrix-visual">
                        <div>
                            [ 2, 5 ] <br> &middot; <br> [ 3, 1 ]
                        </div>
                        <div style="font-size: 2rem;">=</div>
                        <div>
                            (2*3) + (5*1) <br> = <strong>11</strong>
                        </div>
                    </div>
                </div>
            </div>

            <div class="slide">
                <h3>2.2 Data Representation</h3>
                <p>How do we represent non-numeric data?</p>
                <div class="accordion">
                    <div class="acc-item">
                        <h4 onclick="toggleAcc(this)">Images <i class="fas fa-chevron-down"></i></h4>
                        <div class="acc-content">
                            <p>Grayscale images are 2D matrices (0-255). Color images are 3D tensors (Height x Width x 3 Channels [RGB]).</p>
                        </div>
                    </div>
                    <div class="acc-item">
                        <h4 onclick="toggleAcc(this)">Text: One-Hot Encoding <i class="fas fa-chevron-down"></i></h4>
                        <div class="acc-content">
                            <p>Binary vectors. If vocab size is 10,000, "Apple" is a vector with one '1' and 9,999 '0's.</p>
                            <p><strong>Problem:</strong> Creates <span class="term" data-desc="Vectors containing mostly zeros. High dimensionality and memory inefficiency.">Sparse Vectors</span>. No semantic meaning.</p>
                        </div>
                    </div>
                    <div class="acc-item">
                        <h4 onclick="toggleAcc(this)">Text: Embeddings <i class="fas fa-chevron-down"></i></h4>
                        <div class="acc-content">
                            <p>Dense vectors where similar words have similar values (close in vector space).</p>
                            <p>Derived via Matrix Factorization (SVD) or Neural Nets (Word2Vec).</p>
                        </div>
                    </div>
                </div>
            </div>

            <div class="slide">
                <h3>2.3 Dimensionality Reduction (PCA)</h3>
                <p><strong>Principal Component Analysis (PCA)</strong> reduces dataset complexity while keeping the most important information.</p>
                <div class="concept-diagram">
                    <div class="flow-step">1. Standardize Data</div> &rarr; 
                    <div class="flow-step">2. Covariance Matrix</div> &rarr; 
                    <div class="flow-step highlight">3. Eigenvectors & Eigenvalues</div> &rarr; 
                    <div class="flow-step">4. Project to new space</div>
                </div>
                <ul>
                    <li><strong>Eigenvector:</strong> The <em>direction</em> of the spread of data.</li>
                    <li><strong>Eigenvalue:</strong> The <em>magnitude</em> (variance) in that direction.</li>
                    <li><strong>SVD (Singular Value Decomposition):</strong> A matrix factorization technique used to solve PCA mathematically.</li>
                </ul>
            </div>
        </section>

        <section id="mod3" class="module">
            <div class="header-banner">
                <h1>Module 3: ML Algorithms</h1>
                <p>Regression, Classification, Clustering & Evaluation</p>
            </div>

            <div class="slide">
                <h3>3.1 Regression</h3>
                <p>Predicting continuous values. <strong>Linear Regression</strong> fits a line $y = wx + b$.</p>
                <div class="formula">
                    <strong>Cost Function (MSE):</strong> $$ \frac{1}{n} \sum (y_{actual} - y_{pred})^2 $$
                </div>
                <p><strong>Goal:</strong> Minimize MSE using <strong>Ordinary Least Squares (OLS)</strong> or Gradient Descent.</p>
            </div>

            <div class="slide">
                <h3>3.2 Classification</h3>
                <div class="grid-2">
                    <div class="card">
                        <h4>KNN (K-Nearest Neighbors)</h4>
                        <p>Classifies based on majority vote of $K$ nearest points.</p>
                        <p><em>Properties:</em> Lazy Learner, Sensitive to $K$ (Small K = Overfitting), needs feature scaling.</p>
                    </div>
                    <div class="card">
                        <h4>Decision Trees</h4>
                        <p>Splits data to maximize <strong>Information Gain</strong> (reduce Entropy).</p>
                        <p>$$ \text{Entropy} = - \sum p(x) \log p(x) $$</p>
                        <p><strong>Pure Node:</strong> Entropy = 0.</p>
                    </div>
                    <div class="card" style="grid-column: span 2;">
                        <h4>SVM (Support Vector Machine)</h4>
                        <p>Finds a <strong>Hyperplane</strong> that maximizes the <span class="term" data-desc="Distance between the hyperplane and the nearest data points (support vectors) of any class.">Margin</span>.</p>
                        <p><strong>Kernel Trick:</strong> Maps non-linear data to higher dimensions (e.g., RBF Kernel) to make it separable.</p>
                    </div>
                </div>
            </div>

            <div class="slide">
                <h3>3.3 Evaluation Metrics</h3>
                <p>How good is our model?</p>
                <table class="confusion-matrix">
                    <tr><th></th><th>Predicted Positive</th><th>Predicted Negative</th></tr>
                    <tr><th>Actual Positive</th><td class="tp">True Positive (TP)</td><td class="fn">False Negative (FN) <br><small>(Missed detection)</small></td></tr>
                    <tr><th>Actual Negative</th><td class="fp">False Positive (FP) <br><small>(False Alarm)</small></td><td class="tn">True Negative (TN)</td></tr>
                </table>
                <ul style="margin-top:20px;">
                    <li><strong>Accuracy:</strong> $(TP+TN) / Total$. (Bad for imbalanced data).</li>
                    <li><strong>Recall (Sensitivity):</strong> $TP / (TP+FN)$. Essential for medical diagnosis (don't miss a sick person).</li>
                    <li><strong>Precision:</strong> $TP / (TP+FP)$. Essential for spam filters (don't block real emails).</li>
                    <li><strong>F1-Score:</strong> Harmonic mean of Precision & Recall.</li>
                </ul>
            </div>

            <div class="slide">
                <h3>3.4 Clustering (K-Means)</h3>
                <p>An iterative algorithm to partition data into $K$ non-overlapping subgroups.</p>
                <ol>
                    <li>Initialize $K$ centroids randomly.</li>
                    <li><strong>Assign</strong> each point to the closest centroid (Euclidean distance).</li>
                    <li><strong>Update</strong> centroid position to the mean of assigned points.</li>
                    <li>Repeat until centroids stop moving (Convergence).</li>
                </ol>
                <p><em>Challenge:</em> You must define $K$ manually.</p>
            </div>
        </section>

        <section id="mod4" class="module">
            <div class="header-banner">
                <h1>Module 4: Artificial Neural Networks</h1>
                <p>Perceptrons, Activations & Backpropagation</p>
            </div>

            <div class="slide">
                <h3>4.1 The Artificial Neuron</h3>
                <div class="nn-visual">
                    <div class="layer input">
                        <div class="node">x1</div>
                        <div class="node">x2</div>
                    </div>
                    <div class="arrows">&rarr; Weights ($w$) &rarr;</div>
                    <div class="layer hidden">
                        <div class="node sum">$\sum$ + $b$</div>
                    </div>
                    <div class="arrows">&rarr; Activation ($f$) &rarr;</div>
                    <div class="layer output">
                        <div class="node">y</div>
                    </div>
                </div>
                <p>$$ y = f(\sum (x_i \cdot w_i) + b) $$</p>
                <p>A Multi-Layer Perceptron (MLP) has Input, Hidden, and Output layers.</p>
            </div>

            <div class="slide">
                <h3>4.2 Activation Functions</h3>
                <p>Crucial for introducing <strong>Non-Linearity</strong>. Without them, a Neural Net is just Linear Regression.</p>
                <table class="info-table">
                    <tr><th>Function</th><th>Range</th><th>Pros/Cons</th></tr>
                    <tr><td><strong>Sigmoid</strong></td><td>(0, 1)</td><td>Good for probability. <strong>Con:</strong> <span class="term" data-desc="Gradients become extremely small in deep networks, stopping the learning process.">Vanishing Gradient</span>.</td></tr>
                    <tr><td><strong>Tanh</strong></td><td>(-1, 1)</td><td>Zero-centered. Still suffers Vanishing Gradient.</td></tr>
                    <tr><td><strong>ReLU</strong></td><td>[0, $\infty$)</td><td>Fast. Solves Vanishing Gradient. Standard for Hidden Layers.</td></tr>
                    <tr><td><strong>Softmax</strong></td><td>(0, 1)</td><td>Output layer for multi-class classification (Sums to 1).</td></tr>
                </table>
            </div>

            <div class="slide">
                <h3>4.3 Training the Network</h3>
                <ol>
                    <li><strong>Forward Propagation:</strong> Calculate output.</li>
                    <li><strong>Loss Calculation:</strong> Compare with ground truth.</li>
                    <li><strong>Backpropagation:</strong> Calculate gradients (derivatives) to find error contribution of each weight.</li>
                    <li><strong>Gradient Descent:</strong> Update weights ($w_{new} = w_{old} - \alpha \cdot gradient$).</li>
                </ol>
                <p><strong>Terminology:</strong></p>
                <ul>
                    <li><strong>Epoch:</strong> One pass through the full dataset.</li>
                    <li><strong>Batch:</strong> Subset of data processed before updating weights.</li>
                    <li><strong>Iteration:</strong> Number of batches needed to complete one epoch.</li>
                </ul>
            </div>
        </section>

        <section id="mod5" class="module">
            <div class="header-banner">
                <h1>Module 5: Deep Learning</h1>
                <p>CNN, RNN, LSTM & Autoencoders</p>
            </div>

            <div class="slide">
                <h3>5.1 CNN (Convolutional Neural Networks)</h3>
                <p>Specialized for Grid Data (Images). Preserves spatial relationships.</p>
                <div class="concept-diagram">
                    <div class="flow-step">Input Image</div> &rarr; 
                    <div class="flow-step highlight">Conv Layer + ReLU</div> &rarr; 
                    <div class="flow-step warning">Pooling Layer</div> &rarr; 
                    <div class="flow-step">Flatten</div> &rarr; 
                    <div class="flow-step">Fully Connected</div>
                </div>
                <ul>
                    <li><strong>Convolution:</strong> Uses Filters (Kernels) to extract features (Edges, Shapes).</li>
                    <li><strong>Pooling (Max/Average):</strong> Down-sampling. Reduces dimensionality, parameters, and overfitting.</li>
                </ul>
            </div>

            <div class="slide">
                <h3>5.2 RNN & LSTM</h3>
                <p><strong>RNN (Recurrent Neural Network):</strong> Designed for Sequential Data (Time series, Text). Has "memory" (loops).</p>
                <p><em>Problem:</em> <span class="term" data-desc="RNNs struggle to learn dependencies in long sequences because gradients shrink to zero during backpropagation.">Vanishing Gradient</span> leads to short-term memory.</p>
                
                <h4>LSTM (Long Short-Term Memory)</h4>
                <p>Solves RNN's forgetting problem using a <strong>Cell State</strong> and 3 Gates:</p>
                <ol>
                    <li><strong>Forget Gate:</strong> Decides what information to throw away.</li>
                    <li><strong>Input Gate:</strong> Decides what new information to store.</li>
                    <li><strong>Output Gate:</strong> Decides what to output to the next state.</li>
                </ol>
            </div>

            <div class="slide">
                <h3>5.3 Autoencoders</h3>
                <p>Unsupervised neural network for <strong>Data Compression</strong>, Denoising, or Anomaly Detection.</p>
                <div class="nn-visual" style="gap:5px;">
                    <div class="node" style="height:60px;">In</div> &rarr; 
                    <div class="node" style="height:40px;">Enc</div> &rarr; 
                    <div class="node" style="height:20px; background:#f1c40f;">z</div> &rarr; 
                    <div class="node" style="height:40px;">Dec</div> &rarr; 
                    <div class="node" style="height:60px;">Out</div>
                </div>
                <p>The middle layer is the <strong>Bottleneck (z)</strong> (Latent Space). It forces the network to learn efficient patterns.</p>
            </div>
        </section>

        <section id="mod6" class="module">
            <div class="header-banner">
                <h1>Module 6: NLP</h1>
                <p>Natural Language Processing & Transformers</p>
            </div>

            <div class="slide">
                <h3>6.1 Text Preprocessing</h3>
                <ul>
                    <li><strong>Tokenization:</strong> Splitting text into words or sub-words.</li>
                    <li><strong>Stop Words:</strong> Removing common words (the, is, a) that carry little meaning.</li>
                    <li><strong>Stemming:</strong> Cutting words to their root (Running &rarr; Run). Fast but crude.</li>
                    <li><strong>Lemmatization:</strong> Reducing words to dictionary root using grammar. Slower but accurate.</li>
                </ul>
            </div>

            <div class="slide">
                <h3>6.2 Feature Extraction</h3>
                <div class="grid-2">
                    <div class="card">
                        <h4>TF-IDF</h4>
                        <p><em>Term Frequency - Inverse Document Frequency</em></p>
                        <p>Measures how important a word is. High score if word appears frequently in a document but rarely in the corpus (unique keyword).</p>
                    </div>
                    <div class="card">
                        <h4>Word Embeddings (Word2Vec)</h4>
                        <p>Maps words to dense vectors based on context.</p>
                        <p>Captures relationships: <br> $King - Man + Woman \approx Queen$</p>
                    </div>
                </div>
            </div>

            <div class="slide">
                <h3>6.3 Transformers</h3>
                <p>State-of-the-art architecture (e.g., BERT, GPT). Replaced RNNs.</p>
                <p><strong>Key Innovation: Self-Attention Mechanism.</strong></p>
                <p>Allows the model to look at the <em>entire</em> sentence at once and weigh the importance of different words relative to each other (Context).</p>
                <p><strong>Hugging Face:</strong> The leading platform/library for pre-trained Transformer models. Supports pipelines for Sentiment Analysis, Translation, etc.</p>
            </div>
        </section>

        <section id="quiz-area" class="hidden">
            <div class="quiz-container-wrapper">
                <div class="quiz-header">
                    <h1><i class="fas fa-edit"></i> Exam Simulator</h1>
                    <div class="stats-bar">
                        Question: <span id="q-current">1</span>/<span id="q-total">0</span> | Score: <span id="score">0</span>
                    </div>
                </div>
                
                <div id="quiz-card-container">
                    <p style="text-align:center; color:#7f8c8d;">Loading questions...</p>
                </div>

                <div id="quiz-footer">
                    <button id="submit-btn" class="btn-primary hidden" onclick="checkAnswer()">Submit Answer</button>
                    <button id="next-btn" class="btn-secondary hidden" onclick="nextQuestion()">Next Question <i class="fas fa-arrow-right"></i></button>
                </div>
            </div>
        </section>
    </main>

    <div id="term-modal" class="modal">
        <div class="modal-content">
            <span class="close">&times;</span>
            <h3 id="modal-title">Term Definition</h3>
            <p id="modal-desc">Description goes here...</p>
        </div>
    </div>

    <script src="questions.js"></script>
    <script src="script.js"></script>
</body>
</html>